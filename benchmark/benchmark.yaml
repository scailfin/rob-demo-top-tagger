workflow:
    version: '0.3.0'
    inputs:
      files:
        - 'code/'
        - 'data/'
    workflow:
      type: 'serial'
      specification:
        steps:
          - environment: '$[[env_preproc]]'
            commands:
              - '$[[cmd_preproc]]'
          - environment: '$[[env_eval]]'
            commands:
              - '$[[cmd_eval]]'
          - environment: 'toptagger:1.0'
            commands:
              - 'python code/compute-score.py data/evaluate/ results/'
    outputs:
      files:
       - 'results/yProbBest.pkl'
       - 'results/results.json'
       - 'results/analyze.log'
       - 'results/evaluate.log'
       - 'results/preproc.log'
postproc:
    environment: 'toptagger:1.0'
    mount:
        - 'code/'
        - 'data/'
    inputs:
        - 'results/yProbBest.pkl'
    commands:
        - 'python code/plot-roc.py ${in} data/evaluate/labels.pkl ${out}'
    outputs:
        - id: 'ROC.png'
          name: 'ROC Curves for all Algorithms'
          caption: 'ROC curves for all algorithms evaluated on the test sample, shown as the AUC ensemble median of  multiple  trainings. '
          type: 'image/png'
modules:
    - id: 'preproc'
      name: 'Pre-Processing Step'
      index: 0
    - id: 'eval'
      name: 'ML Evaluation Step'
      index: 1
parameters:
    - id: 'env_preproc'
      name: 'Environment (Pre-Processing)'
      datatype: 'string'
      defaultValue: 'toptagger:1.0'
      index: 0
      module: 'preproc'
    - id: 'cmd_preproc'
      name: 'Command  (Pre-Processing)'
      datatype: 'string'
      defaultValue: 'python code/preprocess-dataset.py
        data/test_jets.pkl
        data/preprocess/
        results/'
      index: 1
      module: 'preproc'
    - id: 'env_eval'
      name: 'Environment (ML)'
      datatype: 'string'
      defaultValue: 'toptagger:1.0'
      index: 2
      module: 'eval'
    - id: 'cmd_eval'
      name: 'Command (ML)'
      datatype: 'string'
      defaultValue: 'python code/your_script.py
        results/processed_test_jets.pkl
        data/evaluate/
        results/'
      index: 3
      module: 'eval'
results:
    file: 'results/results.json'
    schema:
        - id: 'bg_reject_outliers'
          name: 'Background rejection (at 50%)'
          type: 'decimal'
        - id: 'bg_reject_std_outliers'
          name: 'Background rejection (STD)'
          type: 'decimal'
        - id: 'aucs_outliers'
          name: 'AUC'
          type: 'decimal'
    orderBy:
        - id: 'bg_reject_outliers'
          sortDesc: true
        - id: 'bg_reject_std_outliers'
          sortDesc: false
